{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning Category\n",
    "1. Supervised learning\n",
    "\n",
    "Ensemble \n",
    "(averaging methods : Bagging, Forest of randomized trees,\n",
    "boosting methods\n",
    ")\n",
    "Neural Nets,\n",
    "\n",
    "\n",
    "2. UnSupervised learning\n",
    "- reinforcement learning\n",
    "- Generative Adversial Network (GAN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning Problem setting\n",
    "\n",
    "\n",
    "1. Regression\n",
    "\n",
    "\n",
    "2. Classification\n",
    "\n",
    "\n",
    "3. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning Pipeline\n",
    "\n",
    "### 1. Preprocessing\n",
    "\n",
    "Data crawling\n",
    "\n",
    "=======================================================================\n",
    "\n",
    "(Dataset transformations)[http://scikit-learn.org/stable/data_transforms.html]\n",
    "\n",
    "scikit-learn provides a library of transformers, which may \n",
    "\n",
    "clean (see Preprocessing data), \n",
    "\n",
    "reduce (see Unsupervised dimensionality reduction), \n",
    "\n",
    "expand (see Kernel Approximation) or generate (see Feature extraction) feature representations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NaN, DataFrame generation, data augmentation\n",
    "\n",
    "### 2. Feature selection - engineering\n",
    "[feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "null hypothesis\n",
    "\n",
    "VarianceThreshold, chi2, mutual information\n",
    "covariance matrix, correlation matrix,\n",
    "pvalue, tvalue,\n",
    "(t-test,\n",
    "ANOVA,\n",
    "\n",
    "=======================================================================\n",
    "\n",
    "[time series analy.](http://www.dodomira.com/2016/04/21/r-%EC%8B%9C%EA%B3%84%EC%97%B4-%EB%B6%84%EC%84%9D-arima/)\n",
    "\n",
    "ARIMA - rolling mean\n",
    "\n",
    "\n",
    "1. Low Variance\n",
    "VarianceThreshold\n",
    "\n",
    "2. Univariate feature selection\n",
    "select function (SelectKBest, SelectPercentile, .. , GenericUnivariateSelect)\n",
    "regression function (f_regression, mutual_info_regression)\n",
    "classification function (chi2, f_classif, mutual_info_classif)\n",
    "\n",
    "parse features selection\n",
    "\n",
    "selectionfrom model:\n",
    "feature_importance_  \n",
    "or\n",
    "coef_\n",
    "gives feature selection\n",
    "\n",
    "\n",
    "3. Recursive Feature Elimination (RFE)\n",
    "\n",
    "4. Decomposition\n",
    "Principal Component Analysis (PCA)\n",
    "NMF\n",
    "\n",
    "\n",
    "5. Topological TDA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3. Model selection - tuning // Hyper parameter tuning\n",
    "[model selection](http://scikit-learn.org/stable/model_selection.html)\n",
    "\n",
    "1. Cross validation\n",
    "(GridSearchCV, RandomizedSearchCV)\n",
    "\n",
    "\n",
    "2. Tuning Hyper parameters\n",
    "A search consists of:\n",
    "\n",
    "    an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "    a parameter space;\n",
    "    a method for searching or sampling candidates;\n",
    "    a cross-validation scheme; and\n",
    "    a score function.\n",
    "\n",
    "\n",
    "[Hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "\n",
    "[Keras+Hyperopt](https://github.com/maxpumperla/hyperas)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Model qunatifying (scoring) and qualiyfying (metrics)\n",
    "There are 3 different approaches to evaluate the quality of predictions of a model:\n",
    "\n",
    "    Estimator score method: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. This is not discussed on this page, but in each estimatorâ€™s documentation.\n",
    "    \n",
    "    Scoring parameter: Model-evaluation tools using cross-validation (such as model_selection.cross_val_score and model_selection.GridSearchCV) rely on an internal scoring strategy. This is discussed in the section The scoring parameter: defining model evaluation rules.\n",
    "    \n",
    "    Metric functions: The metrics module implements functions assessing prediction error for specific purposes. These metrics are detailed in sections on Classification metrics, Multilabel ranking metrics, Regression metrics and Clustering metrics.\n",
    "\n",
    "\n",
    "= metric, loss, score\n",
    "R^2, AIC, BIC\n",
    "\n",
    "confusion matrix : \n",
    "\n",
    "accuracy : from Confusion matrix, (TP+TN)/(P+N)\n",
    "\n",
    "recall : fractions of retrieved instances that are relevant = How complete the results are\n",
    "\n",
    "precision : fractions of relevent instances that are retrieved = How useful the search results are\n",
    "\n",
    "\n",
    "=======================================================================\n",
    "\n",
    "4. model in detail\n",
    "\n",
    ".Support Vector Machine\n",
    "SVC : in short, is good where \n",
    "the number of samples >= the number of features and\n",
    "the number of samples <=>  the number of dimensions\n",
    "\n",
    "okay to use when the number of dimension is much bigger than the number of samples, kernel function is versatile. But it doens't offer probs unless using N fold cross validation.\n",
    "\n",
    "\n",
    ".Ensemble Method\n",
    "Bagging : want to reduce the variance while retaining the bias\n",
    "\n",
    "    Bagging\n",
    "\n",
    "Boosting : want to reduce both variance and bias\n",
    "\n",
    "    AdaBoostClassifier\n",
    "    \n",
    "    GradientBoostingClassifier\n",
    "    \n",
    "\n",
    "Classification with more than 2 classes requires the induction of n_classes regression trees at each iteration, thus, the total number of induced trees equals n_classes * n_estimators. For datasets with a large number of classes we strongly recommend to use RandomForestClassifier as an alternative to GradientBoostingClassifier .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "=======================================================================\n",
    "\n",
    "### 4. Extract Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classify', LinearSVC())\n",
    "])\n",
    "\n",
    "N_FEATURES_OPTIONS = [2, 4, 8]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7), NMF()],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest(chi2)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
